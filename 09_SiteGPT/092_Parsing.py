import os, time
from dotenv import load_dotenv

load_dotenv()

print(f'âœ… {os.path.basename( __file__ )} ì‹¤í–‰ë¨ {time.strftime('%Y-%m-%d %H:%M:%S')}')  # ì‹¤í–‰íŒŒì¼ëª…, í˜„ì¬ì‹œê°„ì¶œë ¥
print(f'\tOPENAI_API_KEY={os.getenv("OPENAI_API_KEY")[:20]}...') # OPENAI_API_KEY í•„ìš”!
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import streamlit as st

from langchain_text_splitters.character import RecursiveCharacterTextSplitter
from langchain_community.vectorstores.faiss import FAISS
from langchain_openai.embeddings.base import OpenAIEmbeddings
from langchain_core.runnables.passthrough import RunnablePassthrough
from langchain_core.runnables.base import RunnableLambda
from langchain_openai.chat_models.base import ChatOpenAI
from langchain_core.prompts.chat import ChatPromptTemplate

from langchain_community.document_loaders.sitemap import SitemapLoader


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸƒ LLM ë¡œì§
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
llm = ChatOpenAI(
    temperature=0.1,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ‡ file load & cache
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# document ì „ì²´ HTML ì„ ê°€ì§„ Beautiful soup object ê°’ì´ ì „ë‹¬ëœë‹¤
# ì—¬ê¸°ì„œ ê²€ìƒ‰(search) í•˜ê±°ë‚˜, HTML element ë“¤ì„ ì œê±°í• ìˆ˜ ìˆë‹¤.
# ì´ í•¨ìˆ˜ëŠ” Document ì— í¬í•¨í•˜ê³  ì‹¶ì€ text ë¥¼ ë¦¬í„´í•´ì•¼ í•œë‹¤
def parse_page(soup):
    header = soup.select_one("#header")  # id='header' ì¸ element 
    footer = soup.select_one("#footer")  # id='footer' ì¸ element
    
    # decompose() í•´ë‹¹ element ë¥¼ HTML ë¬¸ì„œì—ì„œ ì œê±°
    if header:
        header.decompose()
    
    if footer:
        footer.decompose()
    
    # ì „ì²´ í˜ì´ì§€ì—ì„œ header ì™€ footer ë¥¼ ì œê±°í•œ ë‚˜ë¨¸ì§€ text ë¥¼ ë¦¬í„´í•œë‹¤.
    return (
        str(soup.get_text())
        .replace('\n', " ")  # ì¤„ë°”ê¿ˆ ë¬¸ì
        .replace('\xa0', " ") # &nbsp; ë¬¸ì
        .replace('The next chapter of AI is yours.', '')
        )

@st.cache_resource(show_spinner="Fetching URL...")
def load_website(url):

    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        chunk_size=1000,
        chunk_overlap=200,
    )


    loader = SitemapLoader(
        url,
        # load í•˜ê³  ì‹¶ì€ url ë“¤ì„ ë‹´ì€ list, ì´ë•Œ url ì€ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ë‹¤ë£¨ì–´ì§.
        filter_urls=[
            # "https://mistral.ai/news/ai-studio",  # íŠ¹ì • ë‰´ìŠ¤ í˜ì´ì§€ url í•˜ë‚˜ë¥¼ ë³´ì.

            # ì •ê·œí‘œí˜„ì‹ ì‚¬ìš©
            # .*  â† ì „ í›„ì— ë‹¤ë¥¸ ë¬¸ìë“¤ì´ ì˜¬ìˆ˜ëŠ” ìˆì§€ë§Œ
            #  /news/ ë¥¼ í¬í•¨í•˜ëŠ” url ë§Œ ë³¼ìˆ˜ ìˆë‹¤.
            r"^(.*\/news\/).*",              

            # ?! â† negative lookahead   /news/ ë¡¤ í¬í•¨í•˜ì§€ ì•Šì€ urlë§Œ í†µê³¼
            # r"^(?!.*\/news\/).*",              
        ],
        parsing_function=parse_page,
    )
    loader.max_depth = 1    # ê¸°ë³¸ê°’ 10
    loader.headers = {'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Mobile Safari/537.36'}
    docs = loader.load_and_split(text_splitter=splitter)
    return docs


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â­• Streamlit ë¡œì§
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="SiteGPT",
    page_icon="ğŸ–¥ï¸",
)

st.markdown(
"""
    # SiteGPT
            
    Ask questions about the content of a website.
            
    Start by writing the URL of the website on the sidebar.
"""
)

with st.sidebar:
    url = st.text_input(
        "Write down a URL",
        placeholder="https://example.com",
    )

if url:
    # URL ì— XML sitemap ì´ í¬í•¨ë˜ëŠ”ì§€ í™•ì¸í•˜ì.
    if ".xml" not in url:
        with st.sidebar:
            st.error("Please write down a sitemap URL")
    else:
        docs = load_website(url)
        st.write(docs) # í™•ì¸ìš©.
