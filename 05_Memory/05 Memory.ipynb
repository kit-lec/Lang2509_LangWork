{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3388f2cc-89f6-40e9-a7e8-4e9c6c532a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9487809e-cdea-4aaf-9500-2bd879922044",
   "metadata": {},
   "source": [
    "# 기본 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdb98baf-a742-4305-afbb-75820ca55fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9163e80-6462-487a-917b-a8d0200dd742",
   "metadata": {},
   "source": [
    "# Memory\n",
    "\n",
    "https://python.langchain.com/api_reference/langchain/memory.html\n",
    "\n",
    "챗봇으로 하여금 대화(상태)를 '기억'하게끔 한다\n",
    "\n",
    "Memory maintains Chain state, incorporating context from past runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "453fcf0c-4e78-4cbb-aa1a-762023f99184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain 의 memory 계층\n",
    "#  BaseMemory --> BaseChatMemory --> <name>Memory  # Examples: ZepMemory, MotorheadMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df07fbf1-e713-4016-b765-9294ee49c356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI 사에서 제공하는 '기본 API' 도 랭체인 없이 사용 가능.\n",
    "# 메모리 지원하지 않는다.  이전 대화 기억 못함.  stateless 하다!\n",
    "\n",
    "# ChatGPT 서비스 는 '메모리' 기능이 탑재되어 있다.\n",
    "# 챗봇이 이전의 대화 내용과 질문을 기억하고 답할수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7471eca7-4e9b-4312-90a0-137fca5d12a1",
   "metadata": {},
   "source": [
    "# 1.ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b195567f-4256-434e-af4b-098a0a9c30db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationBufferMemory\n",
    "# 대화 내용 '전체'를 저장하는 메모리\n",
    "\n",
    "# 장점: 단순하다\n",
    "\n",
    "# 단점:\n",
    "# => 매번 요청할때마다 '이전 대화 기록 전체' 를 같이 보내야 함.\n",
    "#  그래야 모델이 전에 일어났던 대화를 보고 이해 할수 있다.\n",
    "#  대화내용이 길어질수록 메모리도 계속 커지니까 성능적으로도 & 비용적으로도 비효율적이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f53c506b-d6fd-454f-8fed-22e660116e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory.buffer import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1e7c0cd-7d3c-4bb9-a5b1-47c0963cb0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_16376\\2708910790.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hi!\\nAI: How are you?'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# 직접 save\n",
    "memory.save_context(\n",
    "    {'input': 'Hi!'},   # 유저 입력값\n",
    "    {'output': 'How are you?'}, # AI 가 유저에게 답한 내용 기록\n",
    ")\n",
    "\n",
    "memory.load_memory_variables({})   # inputs= 매개변수가 빈dict\n",
    "\n",
    "\n",
    "# {'history': 'Human: Hi!\\nAI: How are you?'}\n",
    "#  history 가 텍스트다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d664eb9-e170-433c-b0ac-009c74af1272",
   "metadata": {},
   "source": [
    "## return_messages=True\n",
    "history 에  AIMessage 와 HumanMessage 로 저장된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b2a4c92-4ec2-43f5-ad35-2f7865ce017d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[]), return_messages=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1466910a-fafd-41d0-908b-10446f4b4fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi!', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context(\n",
    "    {'input': 'Hi!'},   # 유저 입력값\n",
    "    {'output': 'How are you?'}, # AI 가 유저에게 답한 내용 기록\n",
    ")\n",
    "\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "# {'history': [HumanMessage(content='Hi!', additional_kwargs={}, response_metadata={}),\n",
    "#   AIMessage(content='How are you?', additional_kwargs={}, response_metadata={})]}\n",
    "\n",
    "#  history 가 Message 의 list 다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae0a595b-dfad-4c09-b81d-b03733136acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi!', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Hi!', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context(\n",
    "    {'input': 'Hi!'},   # 유저 입력값\n",
    "    {'output': 'How are you?'}, # AI 가 유저에게 답한 내용 기록\n",
    ")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77e97ca-333b-409d-a95c-4e9d212bbcfa",
   "metadata": {},
   "source": [
    "# 2.ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbf12f00-2e36-41af-b2a7-29449b7fc6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory.buffer_window import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fabbe919-9fcb-49a1-adbb-a5bb4f10db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationBufferWindowMemory 는 대화의 '특정 부분만' 을 저장하는 메모리.\n",
    "\n",
    "# 장점:\n",
    "#   메모리를 특정 크기로 유지할 수 있다!\n",
    "#   따라서 모든 대화 내용을 저장하지 않아도 된다!\n",
    "\n",
    "# 단점:\n",
    "#   챗봇이 전체 대화가 아닌 '최근 대화' 에만 집중하게 된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f02016dc-c558-41af-bc55-280f1f5ab98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_16376\\823005658.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=4,   # 버퍼 윈도우 사이즈.  몇개의 context(메세지)를 저장할지 지정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b7d4de0-4cdf-403d-8cf5-ad5eb91d9285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도우미 함수 준비.\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c0ecfb2-aa89-48b8-bd37-28bfcf27cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동작 확인\n",
    "add_message(\"1\", \"1\")\n",
    "add_message(\"2\", \"2\")\n",
    "add_message(\"3\", \"3\")\n",
    "add_message(\"4\", \"4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f374830b-71ac-400a-9c71-e9795b54c1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='1', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='1', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='4', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23e4fbca-47d9-4a1d-a0d0-4e121493bdb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='5', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='5', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\"5\", \"5\")\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365dbdc8-00e4-4dc8-bc78-3afc40905ef5",
   "metadata": {},
   "source": [
    "# 3.ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "097c2e94-d186-479d-aad6-2c3fcc77eeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationSummaryMemory 는 LLM 을 사용하여 대화의 요약본 (summary) 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "637a5f93-86ac-421f-91aa-640de47f5c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb0e166c-3870-437e-9c3d-893a79807edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory.summary import ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e08b4b32-4ed0-4488-b6e6-40678abc096c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_16376\\2664529100.py:7: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm)  # llm 을 사용하여 요약한다!\n"
     ]
    }
   ],
   "source": [
    "# ConversationSummaryMemory\n",
    "# 메세지를 그대로 저장하는 것이 아니라 Convertaion 의 '요약'을 해준다. LLM 필요\n",
    "\n",
    "# 장점: 대화의 메세지가 많아질수록 요약을 해주어 토큰의 양도 줄어들어 훨씬 경제적인 방법\n",
    "# 단점: LLM 호출 발생\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)  # llm 을 사용하여 요약한다!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f9276b5-0b31-4893-b060-51585f6b3de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_history():\n",
    "    return memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39fa6873-c7ab-4ec7-ae85-e49d2fe1fd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# message 추가\n",
    "add_message(\n",
    "    \"Hi I'm John, I live in South Korea\",    # input\n",
    "    \"Wow that is so cool!\"  # output : AI 답변\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b118210-f1df-4a1d-88a7-d0c8ea7361a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 또 message 추가\n",
    "add_message(\n",
    "    \"South Korea is so pretty\",\n",
    "    \"I wish I could go!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "742bba6a-d9bd-4dc5-94bb-648860cd8a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'John introduces himself as living in South Korea. The AI responds with enthusiasm, finding it cool and expressing a desire to visit because South Korea is so pretty.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c00d5a-8ffe-43d9-9f98-dc4a11e433ca",
   "metadata": {},
   "source": [
    "# 4.ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f867090e-2efe-4074-ba7e-0ddf5a1c87f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory.summary_buffer import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c6c4df0-2d46-458f-a5cc-18a733abe0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_16376\\2068819690.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=150,   # 최대 가용한 메세지 토큰수 (메세지들이 요약되지 전)\n",
    "    return_messages=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c528222d-b387-43ba-930e-607d5de7c716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm John, I live in South Korea\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Wow that is so cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\n",
    "    \"Hi I'm John, I live in South Korea\",    # input\n",
    "    \"Wow that is so cool!\"  # output : AI 답변\n",
    "    )\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14aacab8-3b46-4e59-8057-b87c23c57823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm John, I live in South Korea\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Wow that is so cool!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='South Korea is so pretty', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='I wish I could go!!!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 다시 메세지 추가하고 history 확인\n",
    "add_message(\n",
    "    \"South Korea is so pretty\",\n",
    "    \"I wish I could go!!!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "212157c3-129d-4437-accb-1a93ad32177e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm John, I live in South Korea\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Wow that is so cool!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='South Korea is so pretty', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='I wish I could go!!!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='How far is Korea from Argentina?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I don't know! Super far!\", additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\n",
    "    \"How far is Korea from Argentina?\",\n",
    "    \"I don't know! Super far!\"\n",
    ")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79f780be-4503-4eb1-8a25-10e746ec529f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm John, I live in South Korea\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Wow that is so cool!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='South Korea is so pretty', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='I wish I could go!!!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='How far is Korea from Argentina?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I don't know! Super far!\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='How far is Brazil from Argentina?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I don't know! Super far!\", additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 아래 셀을 여러차레 해보자  약 (3,4번?)\n",
    "# =>실제 '요약'이 발생할때면 Model IO 가 발생하기 때문에 시간이 좀 걸리는게 느껴질거다!\n",
    "add_message(\n",
    "    \"How far is Brazil from Argentina?\",\n",
    "    \"I don't know! Super far!\"\n",
    ")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15b0e71d-be03-4e15-8e48-346f09416de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_token_limit 에 도달하면, 오래된 메세지들이 요약되고 있을 것을 확인할수 있다. (SystemMessage 확인)\n",
    "\n",
    "# ★그러나 '요약' 이라는 과정은 API 를 사용한다는 사실을 명심하세요.\n",
    "# ★'요약' 동작은 비용 지출이 발생되는 부분입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d01da10-d82f-437b-8c86-0e1182ad24d3",
   "metadata": {},
   "source": [
    "# 5.ConversationKGMemory\n",
    "KG : Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f4d6394-9cbe-4f60-a946-6b3caa366ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대화중에 '엔티티'의 knowledge graph 를 형성한다 => 가장 중요한 것들만 추출한 요약본.\n",
    "# knowledge graph 는 history 를 가지고 오지 않는다.  대신 '엔티티' 를 가지고 옴\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88b6a73d-b89c-4ee6-ab10-dc34f7600e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.memory.kg import ConversationKGMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf882e89-2de6-4f00-acab-10b48282a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationKGMemory(\n",
    "    llm=llm,  # 이 또한 LLM 을 사용하는 Memory 다  -> Knowledge Graph 를 만든다. (가장 중요한것만 뽑아낸 요약본)\n",
    "    return_messages=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "745358e3-8584-4a1c-8145-a2a50fdb8f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\n",
    "    \"Hi I'm John, I live in South Korea\",    # input\n",
    "    \"Wow that is so cool!\"  # output : AI 답변\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ddf3d29b-f2aa-4e4e-9af7-da1a9757e1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On John: John lives in South Korea.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 대화의 특정 entity 에 대해 질무ㄴ해보자\n",
    "#  Model IO 발생\n",
    "memory.load_memory_variables({\"input\": \"Who is John\"})   # 빈dict 가 아니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c46e586d-54be-474c-b777-98dbd2484c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"John likes kimchi\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3b2166b-37ae-430f-b0a5-9c42aedc0515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On John: John lives in South Korea. John likes kimchi.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": \"Who is John\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a56f980-94fe-4090-a440-ecedfbbf135a",
   "metadata": {},
   "source": [
    "# 6. 참고: Database 와 integration 된 메모리들"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6034f6f9-dc16-4aab-98ea-8875327f5f28",
   "metadata": {},
   "source": [
    "- **참고: Database 와 integration 된 메모리들**\n",
    "\n",
    "| Memory Class                  | 통합 대상 (Integration)             | 설명                                                           |\n",
    "| ----------------------------- | ------------------------------- | ------------------------------------------------------------ |\n",
    "| `RedisChatMessageHistory`     | **Redis**                       | Redis에 메시지 저장. 빠르고 확장 가능한 저장소.                               |\n",
    "| `SQLChatMessageHistory`       | **SQLite, PostgreSQL 등 SQL DB** | SQL 데이터베이스에 메시지 저장. SQLAlchemy 기반.                           |\n",
    "| `DynamoDBChatMessageHistory`  | **AWS DynamoDB**                | AWS의 NoSQL DB인 DynamoDB에 대화 저장. 서버리스 환경에서 유용.                |\n",
    "| `MongoDBChatMessageHistory`   | **MongoDB**                     | 문서 기반 DB인 MongoDB와 통합하여 대화 저장.                               |\n",
    "| `PostgresChatMessageHistory`  | **PostgreSQL**                  | PostgreSQL 전용 구현 (SQLAlchemy 없이).                            |\n",
    "| `FileChatMessageHistory`      | **Local 파일**                    | JSON 파일로 로컬에 저장. 간단한 로깅에 적합.                                 |\n",
    "| `FirestoreChatMessageHistory` | **Google Firestore**            | Firebase 기반 클라우드 NoSQL DB와 통합.                               |\n",
    "| `ChromaMemory`                | **Chroma (벡터 DB)**              | 벡터 DB에 embedding 형태로 memory 저장. RAG나 유사 검색 기반 memory에 활용 가능. |\n",
    "| `WeaviateMemory`              | **Weaviate**                    | Weaviate 벡터 DB와 통합된 memory 저장.                               |\n",
    "| `QdrantMemory`                | **Qdrant**                      | 벡터 기반 memory 저장소로 Qdrant 사용.                                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e62037-e441-477f-8755-8e50cabbdbeb",
   "metadata": {},
   "source": [
    "# 7.Memory on LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d7174b24-5ac6-47af-84d3-1f12c2d1189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리를 'chain' 에 꽂는 방법.\n",
    "# 두가지 형태의 'chain' 에 각각 꽂는 방법.\n",
    "#    1. off-the-shelf chain  : LangChain 에서 '일반적인 목적'을 수행하는 기본 제공되는 chain\n",
    "#    2. LCEL 사용한 chain : 커스텀 chains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "954a1f39-0b31-4930-9fad-41323cb25f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48be0f66-442c-4159-8723-cb448a9cc920",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_16376\\2707029833.py:8: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(   # prompt | llm\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm, \n",
    "    max_token_limit=80,\n",
    ")\n",
    "\n",
    "chain = LLMChain(   # prompt | llm\n",
    "    llm=llm,\n",
    "    memory=memory,  # 메모리 제공\n",
    "    prompt=PromptTemplate.from_template(\"{question}\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cba54564-413f-4514-b2be-d4acb1009d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'My name is John',\n",
       " 'history': '',\n",
       " 'text': 'Nice to meet you, John! How can I assist you today?'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input={\"question\": \"My name is John\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01347696-7bd3-4836-88a4-b98f213d83d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'I live in Seoul',\n",
       " 'history': 'Human: My name is John\\nAI: Nice to meet you, John! How can I assist you today?',\n",
       " 'text': \", the capital city of South Korea. It is a bustling metropolis with a vibrant culture, delicious food, and a mix of modern skyscrapers and historic palaces. I love exploring the city's neighborhoods, trying new restaurants, and taking in the beautiful views of the Han River. Seoul is a dynamic and exciting place to call home.\"}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input={\"question\": \"I live in Seoul\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3debc5b4-54f3-4711-a90e-f3c8dcd2605e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is my name?',\n",
       " 'history': \"System: The human introduces himself as John. The AI greets John and asks how it can assist him. John mentions that he lives in Seoul.\\nAI: , the capital city of South Korea. It is a bustling metropolis with a vibrant culture, delicious food, and a mix of modern skyscrapers and historic palaces. I love exploring the city's neighborhoods, trying new restaurants, and taking in the beautiful views of the Han River. Seoul is a dynamic and exciting place to call home.\",\n",
       " 'text': \"I'm sorry, I do not have access to that information.\"}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 과연 이름을 기억하고 있을까?\n",
    "chain.invoke(input={'question':'What is my name?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7f782e2c-30ab-4553-8982-35fac43c9482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이름을 모른다고???\n",
    "\n",
    "# chain 을 디버깅해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaced07-d656-4d96-b5bf-ec3e620bca9e",
   "metadata": {},
   "source": [
    "## verbose=\n",
    "chain 을 실행했을때 chain 의 프롬프트와 로그들을 확인할수 있다. (디버깅용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d0e493d5-92e0-4678-bb70-1446ebde3a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mMy name is John\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI live in Seoul\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWhat is my name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is my name?',\n",
       " 'history': \"System: John introduces himself and mentions that he lives in Seoul, the capital city of South Korea. The AI responds by describing Seoul as a bustling metropolis with a vibrant culture, delicious food, and a mix of modern skyscrapers and historic palaces. The AI loves exploring the city's neighborhoods, trying new restaurants, and taking in the beautiful views of the Han River, making Seoul a dynamic and exciting place to call home. The human reveals his name is John, and the AI warmly greets him and continues to praise Seoul for its energy and charm.\",\n",
       " 'text': \"I'm sorry, I do not have access to personal information such as your name.\"}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(\"{question}\"),\n",
    "    verbose=True,  # chain 을 실행했을때 chain 의 프롬프트 로그들을 확인할수 있다. (디버깅용)\n",
    ")\n",
    "\n",
    "chain.invoke(input={'question':\"My name is John\"})\n",
    "chain.invoke(input={'question':\"I live in Seoul\"})\n",
    "chain.invoke(input={'question':\"What is my name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6691d4a3-030a-4a6a-9567-b4c428423dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ↑ chain 의 prompt 로그 들을 확인할 수 있다.\n",
    "# 보다시피 대화의 내역(history) 가 prompt에 계속 추가되진 않는다.\n",
    "\n",
    "# 우리가 원하는 어떤 방식으로 prompt에게 대화 기록(history)을 추가해줘야 한다!\n",
    "\n",
    "# ↓ 하지만! 메모리는 계속 업데이트 된다.  함 보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "79552760-45f5-48a9-b74a-5b567789076b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: John introduces himself and mentions that he lives in Seoul, the capital city of South Korea. The AI responds by describing Seoul as a bustling metropolis with a vibrant culture, delicious food, and a mix of modern skyscrapers and historic palaces. The AI loves exploring the city's neighborhoods, trying new restaurants, and taking in the beautiful views of the Han River, making Seoul a dynamic and exciting place to call home. The human reveals his name is John, and the AI warmly greets him and continues to praise Seoul for its energy and charm.\\nHuman: What is my name?\\nAI: I'm sorry, I do not have access to personal information such as your name.\"}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0463eb60-da02-41cd-883f-3104fb270668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 메모리 내용이 prompt 에 포함되어야 한다!\n",
    "\n",
    "# memory_key= 에 \"chat_history\" 라고 말해주어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2360cf80-6df3-4063-9b8c-ef67055ace89",
   "metadata": {},
   "source": [
    "## memory_key="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "61864dce-749d-4dae-8508-8a7bd0a1c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\", \n",
    ")\n",
    "\n",
    "# ↓ template 안에는 memory 가 history 를 저장하도록 한 곳을 적어주기만 하면 된다\n",
    "# history 까지 담을 괜찮은 템플릿을 준비해보자\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    You:\n",
    "\"\"\"\n",
    "# AI 가 우리의 대화기록을 기억하면서 여기의 question 을 완성할 수 있기를 기대해보자.\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(template),  # <- template 지정\n",
    "    verbose=True,  # 어떤 prompt 가 만들어질까?  두근두근\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cdc3e7d8-61fa-4429-bb02-bf88cdb05ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    \n",
      "    Human:My name is John\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'My name is John',\n",
       " 'chat_history': '',\n",
       " 'text': 'Hello John! How can I assist you today?'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question': \"My name is John\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "653166f0-903e-4277-8099-e40c4432dd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is John\n",
      "AI: Hello John! How can I assist you today?\n",
      "    Human:I live in Seoul\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'I live in Seoul',\n",
       " 'chat_history': 'Human: My name is John\\nAI: Hello John! How can I assist you today?',\n",
       " 'text': \"That's great to hear! How can I assist you with anything related to Seoul or anything else today?\"}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question': \"I live in Seoul\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d702446-2d9c-473e-8b48-e556af6e7029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is John\n",
      "AI: Hello John! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: That's great to hear! How can I assist you with anything related to Seoul or anything else today?\n",
      "    Human:What is my name?\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is my name?',\n",
       " 'chat_history': \"Human: My name is John\\nAI: Hello John! How can I assist you today?\\nHuman: I live in Seoul\\nAI: That's great to hear! How can I assist you with anything related to Seoul or anything else today?\",\n",
       " 'text': 'Your name is John.'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question':'What is my name?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a135eb37-2134-4811-b133-6eaf60c6ac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - 프롬프트 템플릿 안에서 메모리 내용이 들어갈 공간을 준비한다.  (예: chat_history)\n",
    "# - 메모리를 활용할 템플릿은 원하는대로 작성하면 된다.\n",
    "# - Memory 클래스에선 history 를 어디에 꽂을지 지정해준다 (memory_key=)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bd52d1-93f1-4cd4-8854-47f209163dd9",
   "metadata": {},
   "source": [
    "# 8.Chat based Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9aebe5ea-dc80-4103-864e-944b6abec334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory 클래스는 memory 를 두가지 방식으로 출력\n",
    "#  1. 문자열 형태\n",
    "#  2. Message 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "778179ca-3432-4737-81f0-502b001c9a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': \"Human: My name is John\\nAI: Hello John! How can I assist you today?\\nHuman: I live in Seoul\\nAI: That's great to hear! How can I assist you with anything related to Seoul or anything else today?\\nHuman: What is my name?\\nAI: Your name is John.\"}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})\n",
    "# ↓ 결과는 텍스트 형태 (문자열).  프롬프트에 표시되는 방식도 텍스트다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b093b9c5-71c4-468c-b426-e09e5b117fb8",
   "metadata": {},
   "source": [
    "## return_messages=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d67216f7-a176-4b1f-8a32-81158ca3b83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key='chat_history',\n",
    "    return_messages=True,  # history 를 '문자열' 이 아닌 'Message' 로 바꾸어 리턴함.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d91e6c77-9978-44b8-861b-fc2a0a341c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b25826-d5b6-4714-b4b8-01c508c41003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 과연 prompt 에는 어떻게 대화 history 들을 넘겨줄수 있을까?\n",
    "#  단순한 하나의 텍스트가 아니라... Human Message - AI Message - Human Messagbe - AI Message - .... (여러 메세지들)\n",
    "#  심지여 요약본 발생시 System message 도 있을텐데?\n",
    "\n",
    "# prompt 에 이를 위한 공간을 어케 만드나?\n",
    "#  => MessagePlaceHolder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789cbbf4-7e16-4f85-b248-38b86806dff8",
   "metadata": {},
   "source": [
    "## MessagePlaceHolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c2f2b6a6-708c-434f-b810-a3b42397d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f6d326bb-0073-43e1-b262-9cbd177f2547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MessagesPlaceholder 를 활용한 프롬프트 를 작성\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', \"You are a helpful AI talking to a human\"),\n",
    "\n",
    "    MessagesPlaceholder(variable_name='chat_history'),\n",
    "    \n",
    "    ('human', '{question}'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0934f063-d9b1-4ec5-9ad1-4fb7fb1328d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt, \n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b7abd4f4-7c67-4aa7-9072-1ad3bd9ec96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is John\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'My name is John',\n",
       " 'chat_history': [HumanMessage(content='My name is John', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello John! How can I assist you today?', additional_kwargs={}, response_metadata={})],\n",
       " 'text': 'Hello John! How can I assist you today?'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question':\"My name is John\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9dc23fa3-f827-4003-b463-9f5450a6d9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is John\n",
      "AI: Hello John! How can I assist you today?\n",
      "Human: I live in Seoul\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'I live in Seoul',\n",
       " 'chat_history': [HumanMessage(content='My name is John', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello John! How can I assist you today?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='I live in Seoul', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?', additional_kwargs={}, response_metadata={})],\n",
       " 'text': 'Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question':\"I live in Seoul\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "01e7ebfa-2811-428e-afd5-422936468aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is John\n",
      "AI: Hello John! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?\n",
      "Human: What is my name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is my name?',\n",
       " 'chat_history': [HumanMessage(content='My name is John', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello John! How can I assist you today?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='I live in Seoul', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Your name is John.', additional_kwargs={}, response_metadata={})],\n",
       " 'text': 'Your name is John.'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question':\"What is my name?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc1e58c-4103-4665-9df5-bb8c9a726f4b",
   "metadata": {},
   "source": [
    "# LCEL Based Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c8bcd847-0a6e-4270-acdd-fa8f80496e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key='chat_history',\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "013a2c85-0dfe-4c51-85bf-6dda57780917",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "08536bc8-5afc-4ba4-b7b7-1b4a777693ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello John! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 24, 'total_tokens': 34, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CaJNvmoYnC7woBYCpmpE9Nzf0IXwN', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--5548febc-550f-4730-a51c-9c338e31b0ba-0', usage_metadata={'input_tokens': 24, 'output_tokens': 10, 'total_tokens': 34, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "    'chat_history': memory.load_memory_variables({})['chat_history'],\n",
    "    'question': 'My name is John',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4254f680-ed50-4228-a6e4-8b2f3ebab0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 방법도 가능하지만,  Runnable 을 사용하여 호출해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9d261e-d7b3-4552-abb0-6442a021abd7",
   "metadata": {},
   "source": [
    "## RunnablePassThrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8ae87cc8-0d99-488d-a5fd-4b77768faeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.passthrough import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a4b05076-725a-4a83-b0d6-6ab766fbb924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_memory():\n",
    "    return memory.load_memory_variables({})['chat_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8fb887c8-12ce-4d50-a7ff-8d8b7f46a730",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_memory() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 위 chain 을 호출하면 load_memory() 가 가장 먼처 호출.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#  호출 결과가 chat_history 키값에 담겨 prompt 로 전달.\u001b[39;00m\n\u001b[32m      5\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m \u001b[38;5;66;03m#     \"question\": \"My name is John\",\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# })\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMy name is John\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\LANG2509\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3244\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3242\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3243\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3244\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3245\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3246\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\LANG2509\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\passthrough.py:530\u001b[39m, in \u001b[36mRunnableAssign.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    525\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    528\u001b[39m     **kwargs: Any,\n\u001b[32m    529\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\LANG2509\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2092\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   2088\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   2089\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   2090\u001b[39m         output = cast(\n\u001b[32m   2091\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m2092\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2093\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2094\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2095\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2096\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2097\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2098\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2099\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   2100\u001b[39m         )\n\u001b[32m   2101\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2102\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\LANG2509\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:430\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    429\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\LANG2509\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\passthrough.py:516\u001b[39m, in \u001b[36mRunnableAssign._invoke\u001b[39m\u001b[34m(self, value, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m    511\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mThe input to RunnablePassthrough.assign() must be a dict.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)  \u001b[38;5;66;03m# noqa: TRY004\u001b[39;00m\n\u001b[32m    514\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    515\u001b[39m     **value,\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m     **\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    521\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\LANG2509\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4001\u001b[39m, in \u001b[36mRunnableParallel.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3996\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m   3997\u001b[39m         futures = [\n\u001b[32m   3998\u001b[39m             executor.submit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[32m   3999\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps.items()\n\u001b[32m   4000\u001b[39m         ]\n\u001b[32m-> \u001b[39m\u001b[32m4001\u001b[39m         output = {key: \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[32m   4002\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   4003\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\LANG2509\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3985\u001b[39m, in \u001b[36mRunnableParallel.invoke.<locals>._invoke_step\u001b[39m\u001b[34m(step, input_, config, key)\u001b[39m\n\u001b[32m   3979\u001b[39m child_config = patch_config(\n\u001b[32m   3980\u001b[39m     config,\n\u001b[32m   3981\u001b[39m     \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[32m   3982\u001b[39m     callbacks=run_manager.get_child(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmap:key:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m),\n\u001b[32m   3983\u001b[39m )\n\u001b[32m   3984\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m-> \u001b[39m\u001b[32m3985\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3986\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3987\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3988\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchild_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3989\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\LANG2509\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5025\u001b[39m, in \u001b[36mRunnableLambda.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5010\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invoke this ``Runnable`` synchronously.\u001b[39;00m\n\u001b[32m   5011\u001b[39m \n\u001b[32m   5012\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   5022\u001b[39m \n\u001b[32m   5023\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5024\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfunc\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m5025\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5026\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5027\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5028\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5029\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5030\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5031\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mCannot invoke a coroutine function synchronously.Use `ainvoke` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5032\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\LANG2509\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2092\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   2088\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   2089\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   2090\u001b[39m         output = cast(\n\u001b[32m   2091\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m2092\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2093\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2094\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2095\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2096\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2097\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2098\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2099\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   2100\u001b[39m         )\n\u001b[32m   2101\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2102\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\LANG2509\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:430\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    429\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\LANG2509\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4882\u001b[39m, in \u001b[36mRunnableLambda._invoke\u001b[39m\u001b[34m(self, input_, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   4880\u001b[39m                 output = chunk\n\u001b[32m   4881\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4882\u001b[39m     output = \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4883\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   4884\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4885\u001b[39m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[32m   4886\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\LANG2509\\LangWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:430\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    429\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: load_memory() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm\n",
    "\n",
    "# 위 chain 을 호출하면 load_memory() 가 가장 먼처 호출.\n",
    "#  호출 결과가 chat_history 키값에 담겨 prompt 로 전달.\n",
    "\n",
    "# 이는 마치\n",
    "# chain.invoke({\n",
    "#     \"chat_history\": load_memory(),  <- 요렇게 한것과 동일하다.\n",
    "#     \"question\": \"My name is John\",\n",
    "# })\n",
    "\n",
    "chain.invoke({\n",
    "    \"question\": \"My name is John\",\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "939fbbc8-38ff-47a4-9a8f-1e1b75ad0e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎃load_memory() {'question': 'My name is John'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello John! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 24, 'total_tokens': 34, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CaJUlrOVpGgvpycaEOJRzwOUdKkjJ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--158b5305-f850-4b1e-a161-c15bdbd97dc1-0', usage_metadata={'input_tokens': 24, 'output_tokens': 10, 'total_tokens': 34, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 도대체 어떤값이 load_memory() 에 전달되는지 확인해보자!\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def load_memory(input):\n",
    "  print(\"🎃load_memory()\", input)  # 확인해보자!!\n",
    "  return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm\n",
    "\n",
    "chain.invoke({\n",
    "    \"question\": \"My name is John\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc02a94-db32-438a-a7fa-aa7db9d4b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load_memory() {'question': 'My name is John'}\n",
    "\n",
    "체인에 있는 모든 컴포넌트는 input 을 받을거고, 또 output 을 줄거다.\n",
    "이게 랭체인의 핵심이다.  모든 것이 input을 얻을거고, 그 후엔 output 을 줄거다.\n",
    "\n",
    "\n",
    "chain.invoke(\n",
    "  {     <--  이 dict 는 chain 의 첫번째 아이템의 input 이 되는거다.\n",
    "             바로 그게 load_memory() 의 input 이 된것이다!   규칙이다!\n",
    "    \"question\": \"My name is John\",\n",
    "  }\n",
    ")\n",
    "\n",
    "이후 load_memory() 를 실행히킨 결과로 얻은것이 chat_history 속성으로 들어가\n",
    "체인의 다음 요소 로 전달되는 것이다.\n",
    "\n",
    "RunnablePassThrough 는 최초 전달받은 'question' 을 체인의 다음요소 까지 전달 (key 값 변경 없이!)\n",
    "\n",
    "\"\"\"\n",
    "None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3e9412-1abb-4b89-a3b0-f215a53348cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "여기서 또 한가지 문제는, chain 호출후 그 결과를 다시 메모리에 저장해야 한다는 것이다.\n",
    "(지금은 메모리 관리를 수동으로 하고 있다.)\n",
    "\n",
    "↓ 여기서 가장 좋은 방법은 체인을 호출하는 함수를 만들어 보는 것이다.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cb215a2a-d52d-4d7d-a541-dd615965d012",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def load_memory(input):\n",
    "  print(\"🎃load_memory()\", input)\n",
    "  return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm\n",
    "\n",
    "# 체인 호출 함수를 직접 만들어 보자\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    # 사용자 질문(question) 과 체인 호출 결과 (result) 를 메모리에 저장\n",
    "    memory.save_context(\n",
    "        {'input': question},\n",
    "        {'output': result.content},\n",
    "    )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7c2d36bd-1165-4010-b9f0-123fc66c1aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎃load_memory() {'question': 'My name is John'}\n",
      "content='Hello John! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 24, 'total_tokens': 34, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CaJcBJckXU0OuzVZEOCalnF5VmGrQ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--f760089e-6a31-4e6f-8b88-44987432b8c9-0' usage_metadata={'input_tokens': 24, 'output_tokens': 10, 'total_tokens': 34, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"My name is John\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d4df47de-b909-4f02-b95c-5cf1629741bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎃load_memory() {'question': 'What is my name?'}\n",
      "content='Your name is John.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 47, 'total_tokens': 52, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CaJcZs7DRWkJ2QVLeUx4zvc7OfLNU', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--7b2fadf9-47fc-458b-bcdc-69290eae2c7d-0' usage_metadata={'input_tokens': 47, 'output_tokens': 5, 'total_tokens': 52, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5993ad72-b318-4c08-887d-c25249fd79c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='My name is John', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello John! How can I assist you today?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Your name is John.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578aa061-44a5-4377-804a-4f9a0e872481",
   "metadata": {},
   "source": [
    "## memory_key= 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65316d33-4525-4bc9-a230-91f88fbe9c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory = ConversationSummaryBufferMemory(\n",
    "#     llm=llm,\n",
    "#     max_token_limit=120,\n",
    "#     memory_key=\"chat_history\",  <-- 이제 이것도 필요하지 않을거다\n",
    "#                   memory_key= 의 기본값은 \"history\" 이기 때문에\n",
    "#                   모든 chat_history 를 이걸로 대체할수 있다.\n",
    "#     return_messages=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8b814e7d-96f9-4644-be52-c74b15c67416",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    # memory_key= 삭제.  기본값 'history'\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),  # variable_name= 값을 기본값 'history'\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def load_memory(input):\n",
    "  print(\"🎃load_memory()\", input)\n",
    "  return memory.load_memory_variables({})[\"history\"]  # 변경!\n",
    "\n",
    "                                    # ↓ 변경!\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context(\n",
    "        {'input': question},\n",
    "        {'output': result.content},\n",
    "    )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2ee83d40-83f7-42be-a7a7-73c32a19164e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎃load_memory() {'question': 'My name is John'}\n",
      "content='Hello John! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 24, 'total_tokens': 34, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CaJi98UaPBZMi9r5Yb5EfMTKxbloY', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--1cc1b101-ae98-43ab-a6d5-3566f1ac3fb9-0' usage_metadata={'input_tokens': 24, 'output_tokens': 10, 'total_tokens': 34, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"My name is John\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cb02e3ac-98d5-4763-98d4-445394ea521b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎃load_memory() {'question': 'What is my name?'}\n",
      "content='Your name is John.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 47, 'total_tokens': 52, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CaJiMY4g1H0yLsXATqqf8k27mJHz9', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--37acd837-46ae-4e56-a223-2030aa22766b-0' usage_metadata={'input_tokens': 47, 'output_tokens': 5, 'total_tokens': 52, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4f4341-8aea-46e8-ab4e-998c46d6bb30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350edfc4-09ac-47a2-a085-58fe1e2efe1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2c4262-c0d0-4aba-b337-bd88d661ca76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bece7d2-988e-490e-a4ae-34e66cbe8c18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec608e2-e61a-4923-a65e-8d0df2d71184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a793c23a-5158-4132-9a14-b7ecfc1d8753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
